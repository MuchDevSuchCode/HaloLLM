[package]
name = "halollm"
version = "0.1.0"
edition = "2021"

[dependencies]
axum = "0.7"
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
# The native Llama.cpp inference wrapper with Vulkan support
llama_cpp_rs = { version = "0.3", optional = true }

[features]
default = []
vulkan = ["llama_cpp_rs/vulkan"]